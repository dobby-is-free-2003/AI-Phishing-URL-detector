# -*- coding: utf-8 -*-
"""AI-Phishing URL detector

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ikbq4Z40n5a5vRM8pGd-pWvjaY71RUMR
"""

!pip install -q scikit-learn matplotlib seaborn plotly tldextract

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import urllib.parse
from urllib.parse import urlparse
import tldextract
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import StandardScaler
import pickle
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class URLFeatureExtractor:
    """Extract features from URLs for phishing detection"""

    def __init__(self):
        self.suspicious_words = [
            'secure', 'account', 'update', 'confirm', 'verify', 'login', 'signin',
            'banking', 'paypal', 'ebay', 'amazon', 'microsoft', 'apple', 'google',
            'suspended', 'limited', 'expired', 'click', 'urgent', 'immediate'
        ]

    def extract_features(self, url):
        """Extract comprehensive features from a URL"""
        features = {}

        try:
            parsed = urlparse(url)
            extracted = tldextract.extract(url)

            # Basic URL structure features
            features['url_length'] = len(url)
            features['domain_length'] = len(parsed.netloc)
            features['path_length'] = len(parsed.path)
            features['query_length'] = len(parsed.query) if parsed.query else 0
            features['fragment_length'] = len(parsed.fragment) if parsed.fragment else 0

            # Count features
            features['num_dots'] = url.count('.')
            features['num_hyphens'] = url.count('-')
            features['num_underscores'] = url.count('_')
            features['num_slashes'] = url.count('/')
            features['num_questionmarks'] = url.count('?')
            features['num_equals'] = url.count('=')
            features['num_ands'] = url.count('&')
            features['num_at'] = url.count('@')
            features['num_digits'] = sum(c.isdigit() for c in url)

            # Domain features
            features['subdomain_count'] = len(extracted.subdomain.split('.')) if extracted.subdomain else 0
            features['is_ip'] = 1 if re.match(r'^\d+\.\d+\.\d+\.\d+', parsed.netloc) else 0
            features['has_port'] = 1 if ':' in parsed.netloc and not parsed.netloc.startswith('[') else 0

            # Suspicious patterns
            features['has_suspicious_words'] = sum(1 for word in self.suspicious_words if word in url.lower())
            features['has_shortening_service'] = 1 if any(service in parsed.netloc.lower()
                                                        for service in ['bit.ly', 'tinyurl', 'short', 'tiny']) else 0

            # HTTPS and security features
            features['is_https'] = 1 if parsed.scheme == 'https' else 0
            features['has_www'] = 1 if 'www.' in parsed.netloc else 0

            # Path and query features
            features['path_depth'] = len([x for x in parsed.path.split('/') if x])
            features['query_params'] = len(parsed.query.split('&')) if parsed.query else 0

            # Entropy (randomness) of domain
            domain = extracted.domain
            if domain:
                prob = [float(domain.count(c)) / len(domain) for c in dict.fromkeys(list(domain))]
                features['domain_entropy'] = -sum([p * np.log2(p) for p in prob if p > 0])
            else:
                features['domain_entropy'] = 0

        except Exception as e:
            # If URL parsing fails, return default features
            features = {key: 0 for key in [
                'url_length', 'domain_length', 'path_length', 'query_length', 'fragment_length',
                'num_dots', 'num_hyphens', 'num_underscores', 'num_slashes', 'num_questionmarks',
                'num_equals', 'num_ands', 'num_at', 'num_digits', 'subdomain_count', 'is_ip',
                'has_port', 'has_suspicious_words', 'has_shortening_service', 'is_https',
                'has_www', 'path_depth', 'query_params', 'domain_entropy'
            ]}

        return features

# Load and prepare the dataset
print("üìä Loading Phishing Dataset...")
try:
    import kagglehub
    path = kagglehub.dataset_download("eswarchandt/phishing-website-detector")
    df = pd.read_csv(f'{path}/phishing.csv')
    print(f"‚úÖ Dataset loaded successfully! Shape: {df.shape}")
except:
    # Fallback: create a sample dataset if Kaggle dataset is not available
    print("‚ö†Ô∏è Kaggle dataset not available, creating sample dataset...")

    # Sample URLs for demonstration
    legitimate_urls = [
        "https://www.google.com",
        "https://github.com/user/repo",
        "https://stackoverflow.com/questions",
        "https://www.amazon.com/products",
        "https://www.wikipedia.org/wiki/page",
        "https://docs.python.org/3/",
        "https://www.youtube.com/watch",
        "https://mail.google.com/mail",
        "https://www.linkedin.com/in/profile",
        "https://www.reddit.com/r/programming"
    ] * 50  # Repeat to get more samples

    phishing_urls = [
        "http://secure-paypal-update.suspicious-domain.com/verify",
        "https://amazon-security-alert.fake-site.net/login",
        "http://192.168.1.1/banking/update-account",
        "https://bit.ly/fake-microsoft-login",
        "http://urgent-account-verification.phishing.com/click-here",
        "https://google-security-team.malicious.org/suspended",
        "http://ebay-account-limited.fake.com/confirm-identity",
        "https://apple-id-locked.suspicious.net/unlock-now",
        "http://your-account-expired.phishing.org/renew",
        "https://click-here-immediately.scam.com/urgent-action"
    ] * 50  # Repeat to get more samples

    # Create DataFrame
    urls = legitimate_urls + phishing_urls
    labels = [0] * len(legitimate_urls) + [1] * len(phishing_urls)
    df = pd.DataFrame({'url': urls, 'class': labels})

# Initialize feature extractor
feature_extractor = URLFeatureExtractor()

print("üîß Extracting features from URLs...")
if 'url' in df.columns:
    url_column = 'url'
elif 'URL' in df.columns:
    url_column = 'URL'
else:
    # Assume we need to create URLs from existing features
    # This is a workaround for datasets that don't have actual URLs
    print("‚ö†Ô∏è No URL column found. Creating synthetic URLs from features...")
    df['url'] = df.apply(lambda row: f"http://example{''.join(str(v) for v in row.values[:5])}.com", axis=1)
    url_column = 'url'

# Extract features for all URLs
feature_list = []
for url in df[url_column]:
    features = feature_extractor.extract_features(str(url))
    feature_list.append(features)

# Create feature DataFrame
X_features = pd.DataFrame(feature_list)
y = df['class'].apply(lambda x: 1 if x == 1 else 0)  # Ensure binary labels

print(f"‚úÖ Feature extraction completed! Features shape: {X_features.shape}")
print(f"üìä Class distribution: {y.value_counts().to_dict()}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_features, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nü§ñ Training Models...")

# Train multiple models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)
}

results = {}
trained_models = {}

for name, model in models.items():
    print(f"Training {name}...")

    if name == 'Logistic Regression':
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    results[name] = {
        'accuracy': accuracy,
        'predictions': y_pred,
        'probabilities': y_pred_proba,
        'classification_report': classification_report(y_test, y_pred)
    }
    trained_models[name] = model

    print(f"‚úÖ {name} Accuracy: {accuracy:.4f}")

# Choose best model (Random Forest typically performs better)
best_model_name = max(results.keys(), key=lambda k: results[k]['accuracy'])
best_model = trained_models[best_model_name]
print(f"\nüèÜ Best Model: {best_model_name} (Accuracy: {results[best_model_name]['accuracy']:.4f})")

# Create visualizations
print("\nüìä Creating Visualizations...")

# Set up the plotting area
fig = plt.figure(figsize=(20, 15))

# 1. Model Comparison
plt.subplot(3, 4, 1)
model_names = list(results.keys())
accuracies = [results[name]['accuracy'] for name in model_names]
bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightcoral'])
plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')

# 2. Confusion Matrix
plt.subplot(3, 4, 2)
cm = confusion_matrix(y_test, results[best_model_name]['predictions'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Legitimate', 'Phishing'],
            yticklabels=['Legitimate', 'Phishing'])
plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')

# 3. ROC Curve
plt.subplot(3, 4, 3)
for name in results.keys():
    fpr, tpr, _ = roc_curve(y_test, results[name]['probabilities'])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves', fontsize=14, fontweight='bold')
plt.legend()

# 4. Feature Importance (for Random Forest)
plt.subplot(3, 4, 4)
if best_model_name == 'Random Forest':
    feature_importance = pd.DataFrame({
        'feature': X_features.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False).head(10)

    plt.barh(range(len(feature_importance)), feature_importance['importance'])
    plt.yticks(range(len(feature_importance)), feature_importance['feature'])
    plt.xlabel('Importance')
    plt.title('Top 10 Feature Importances', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()

# 5. URL Length Distribution
plt.subplot(3, 4, 5)
legitimate_lengths = X_features[y == 0]['url_length']
phishing_lengths = X_features[y == 1]['url_length']
plt.hist(legitimate_lengths, alpha=0.7, label='Legitimate', bins=30, color='green')
plt.hist(phishing_lengths, alpha=0.7, label='Phishing', bins=30, color='red')
plt.xlabel('URL Length')
plt.ylabel('Frequency')
plt.title('URL Length Distribution', fontsize=14, fontweight='bold')
plt.legend()

# 6. Domain Length vs Suspicious Words
plt.subplot(3, 4, 6)
colors = ['green' if label == 0 else 'red' for label in y]
plt.scatter(X_features['domain_length'], X_features['has_suspicious_words'],
           c=colors, alpha=0.6)
plt.xlabel('Domain Length')
plt.ylabel('Number of Suspicious Words')
plt.title('Domain Length vs Suspicious Words', fontsize=14, fontweight='bold')

# 7. HTTPS Usage
plt.subplot(3, 4, 7)
https_data = pd.crosstab(y, X_features['is_https'])
https_data.plot(kind='bar', ax=plt.gca(), color=['lightcoral', 'skyblue'])
plt.title('HTTPS Usage by Class', fontsize=14, fontweight='bold')
plt.xlabel('Class (0=Legitimate, 1=Phishing)')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(['HTTP', 'HTTPS'])

# 8. Number of Dots Distribution
plt.subplot(3, 4, 8)
legitimate_dots = X_features[y == 0]['num_dots']
phishing_dots = X_features[y == 1]['num_dots']
plt.hist(legitimate_dots, alpha=0.7, label='Legitimate', bins=20, color='green')
plt.hist(phishing_dots, alpha=0.7, label='Phishing', bins=20, color='red')
plt.xlabel('Number of Dots')
plt.ylabel('Frequency')
plt.title('Number of Dots Distribution', fontsize=14, fontweight='bold')
plt.legend()

# 9. Subdomain Count
plt.subplot(3, 4, 9)
subdomain_data = pd.crosstab(y, X_features['subdomain_count'])
subdomain_data.plot(kind='bar', ax=plt.gca())
plt.title('Subdomain Count by Class', fontsize=14, fontweight='bold')
plt.xlabel('Class (0=Legitimate, 1=Phishing)')
plt.ylabel('Count')
plt.xticks(rotation=0)

# 10. Path Depth Distribution
plt.subplot(3, 4, 10)
legitimate_depth = X_features[y == 0]['path_depth']
phishing_depth = X_features[y == 1]['path_depth']
plt.hist(legitimate_depth, alpha=0.7, label='Legitimate', bins=15, color='green')
plt.hist(phishing_depth, alpha=0.7, label='Phishing', bins=15, color='red')
plt.xlabel('Path Depth')
plt.ylabel('Frequency')
plt.title('Path Depth Distribution', fontsize=14, fontweight='bold')
plt.legend()

# 11. Class Distribution Pie Chart
plt.subplot(3, 4, 11)
class_counts = y.value_counts()
plt.pie(class_counts.values, labels=['Legitimate', 'Phishing'],
        colors=['lightgreen', 'lightcoral'], autopct='%1.1f%%')
plt.title('Dataset Class Distribution', fontsize=14, fontweight='bold')

# 12. Prediction Confidence Distribution
plt.subplot(3, 4, 12)
probabilities = results[best_model_name]['probabilities']
plt.hist(probabilities, bins=20, alpha=0.7, color='purple')
plt.xlabel('Prediction Confidence')
plt.ylabel('Frequency')
plt.title('Model Confidence Distribution', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# Print detailed results
print("\n" + "="*60)
print("üéØ DETAILED RESULTS")
print("="*60)

for name, result in results.items():
    print(f"\nüìä {name} Results:")
    print(f"Accuracy: {result['accuracy']:.4f}")
    print("\nClassification Report:")
    print(result['classification_report'])

# Feature statistics
print("\nüìà FEATURE STATISTICS")
print("="*40)
print(f"Total features extracted: {len(X_features.columns)}")
print(f"Training samples: {len(X_train)}")
print(f"Testing samples: {len(X_test)}")

print("\nTop 10 most important features:")
if best_model_name == 'Random Forest':
    feature_importance = pd.DataFrame({
        'feature': X_features.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(feature_importance.head(10).to_string(index=False))

# Save the models and components
print("\nüíæ Saving models and components...")
with open('best_phishing_model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

with open('feature_extractor.pkl', 'wb') as f:
    pickle.dump(feature_extractor, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print("‚úÖ Models saved successfully!")

# URL Prediction Function
def predict_url_safety(url, model=None, extractor=None, scaler=None):
    """
    Predict if a URL is phishing or legitimate
    """
    if model is None:
        model = best_model
    if extractor is None:
        extractor = feature_extractor
    if scaler is None and best_model_name == 'Logistic Regression':
        scaler = globals().get('scaler')

    try:
        # Extract features
        features = extractor.extract_features(url)
        feature_vector = np.array(list(features.values())).reshape(1, -1)

        # Scale if using Logistic Regression
        if best_model_name == 'Logistic Regression' and scaler is not None:
            feature_vector = scaler.transform(feature_vector)

        # Predict
        prediction = model.predict(feature_vector)[0]
        probability = model.predict_proba(feature_vector)[0]

        result = {
            'url': url,
            'prediction': 'Phishing ‚ö†Ô∏è' if prediction == 1 else 'Legitimate ‚úÖ',
            'confidence': max(probability),
            'phishing_probability': probability[1],
            'legitimate_probability': probability[0]
        }

        return result

    except Exception as e:
        return {
            'url': url,
            'prediction': 'Error in prediction',
            'error': str(e)
        }

# Test the prediction function
print("\nüß™ TESTING URL PREDICTION")
print("="*50)

test_urls = [
    "https://www.google.com",
    "http://secure-paypal-update.suspicious-domain.com/verify",
    "https://github.com/microsoft/vscode",
    "http://192.168.1.1/banking/update-account",
    "https://stackoverflow.com/questions/python"
]

for url in test_urls:
    result = predict_url_safety(url)
    if 'error' not in result:
        print(f"URL: {result['url']}")
        print(f"Prediction: {result['prediction']}")
        print(f"Confidence: {result['confidence']:.3f}")
        print(f"Phishing Probability: {result['phishing_probability']:.3f}")
        print("-" * 50)
    else:
        print(f"Error predicting {url}: {result['error']}")

print("\nüéâ Phishing Detection System Complete!")
print("‚úÖ Models trained and saved")
print("‚úÖ Visualizations generated")
print("‚úÖ Prediction function ready")
print("\nUse predict_url_safety(url) to check any URL!")



!pip install -q scikit-learn matplotlib seaborn plotly tldextract requests

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import urllib.parse
from urllib.parse import urlparse
import tldextract
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import StandardScaler
import pickle
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('default')
sns.set_palette("husl")

class AdvancedURLFeatureExtractor:
    """Advanced feature extraction for phishing detection"""

    def __init__(self):
        self.suspicious_words = [
            'secure', 'account', 'update', 'confirm', 'verify', 'login', 'signin', 'bank',
            'banking', 'paypal', 'ebay', 'amazon', 'microsoft', 'apple', 'google', 'facebook',
            'suspended', 'limited', 'expired', 'click', 'urgent', 'immediate', 'alert',
            'warning', 'action', 'required', 'suspend', 'verify', 'locked', 'security'
        ]

        self.shortening_services = [
            'bit.ly', 'tinyurl.com', 't.co', 'goo.gl', 'ow.ly', 'short.link',
            'tiny.cc', 'is.gd', 'buff.ly', 'adf.ly'
        ]

        self.legitimate_domains = [
            'google.com', 'facebook.com', 'youtube.com', 'amazon.com', 'wikipedia.org',
            'twitter.com', 'instagram.com', 'linkedin.com', 'reddit.com', 'github.com',
            'stackoverflow.com', 'microsoft.com', 'apple.com', 'netflix.com'
        ]

    def calculate_entropy(self, text):
        """Calculate Shannon entropy of text"""
        if not text:
            return 0
        prob = [float(text.count(c))/len(text) for c in dict.fromkeys(list(text))]
        entropy = -sum([p * np.log2(p) for p in prob if p > 0])
        return entropy

    def extract_features(self, url):
        """Extract comprehensive features from URL"""
        features = {}

        try:
            # Clean URL
            url = str(url).strip()
            parsed = urlparse(url)
            extracted = tldextract.extract(url)

            # 1. Length-based features
            features['url_length'] = len(url)
            features['domain_length'] = len(parsed.netloc)
            features['path_length'] = len(parsed.path)
            features['query_length'] = len(parsed.query) if parsed.query else 0
            features['fragment_length'] = len(parsed.fragment) if parsed.fragment else 0

            # 2. Character count features
            features['num_dots'] = url.count('.')
            features['num_hyphens'] = url.count('-')
            features['num_underscores'] = url.count('_')
            features['num_slashes'] = url.count('/')
            features['num_questionmarks'] = url.count('?')
            features['num_equals'] = url.count('=')
            features['num_ands'] = url.count('&')
            features['num_at'] = url.count('@')
            features['num_digits'] = sum(c.isdigit() for c in url)
            features['num_special_chars'] = sum(not c.isalnum() for c in url)

            # 3. Domain analysis
            domain = extracted.domain + '.' + extracted.suffix if extracted.suffix else extracted.domain
            features['subdomain_count'] = len(extracted.subdomain.split('.')) if extracted.subdomain else 0
            features['is_ip'] = 1 if re.match(r'^\d+\.\d+\.\d+\.\d+', parsed.netloc.split(':')[0]) else 0
            features['has_port'] = 1 if ':' in parsed.netloc and not parsed.netloc.startswith('[') else 0

            # 4. Suspicious patterns
            url_lower = url.lower()
            features['suspicious_word_count'] = sum(1 for word in self.suspicious_words if word in url_lower)
            features['has_shortening_service'] = 1 if any(service in parsed.netloc.lower() for service in self.shortening_services) else 0
            features['has_legitimate_domain'] = 1 if any(legit in domain.lower() for legit in self.legitimate_domains) else 0

            # 5. Security indicators
            features['is_https'] = 1 if parsed.scheme == 'https' else 0
            features['has_www'] = 1 if 'www.' in parsed.netloc else 0

            # 6. URL structure
            features['path_depth'] = len([x for x in parsed.path.split('/') if x])
            features['query_params_count'] = len(parsed.query.split('&')) if parsed.query else 0
            features['has_fragment'] = 1 if parsed.fragment else 0

            # 7. Entropy and randomness
            features['domain_entropy'] = self.calculate_entropy(extracted.domain) if extracted.domain else 0
            features['path_entropy'] = self.calculate_entropy(parsed.path) if parsed.path else 0
            features['url_entropy'] = self.calculate_entropy(url)

            # 8. Advanced patterns
            features['has_hex_chars'] = 1 if re.search(r'[0-9a-f]{8,}', url_lower) else 0
            features['contains_ip'] = 1 if re.search(r'\d+\.\d+\.\d+\.\d+', url) else 0
            features['abnormal_subdomain'] = 1 if features['subdomain_count'] > 3 else 0
            features['long_domain'] = 1 if features['domain_length'] > 20 else 0
            features['redirect_patterns'] = url.count('redirect') + url.count('redir') + url.count('forward')

            # 9. Ratio features
            if len(url) > 0:
                features['digit_ratio'] = features['num_digits'] / len(url)
                features['special_char_ratio'] = features['num_special_chars'] / len(url)
            else:
                features['digit_ratio'] = 0
                features['special_char_ratio'] = 0

        except Exception as e:
            # Return default features if parsing fails
            feature_names = [
                'url_length', 'domain_length', 'path_length', 'query_length', 'fragment_length',
                'num_dots', 'num_hyphens', 'num_underscores', 'num_slashes', 'num_questionmarks',
                'num_equals', 'num_ands', 'num_at', 'num_digits', 'num_special_chars',
                'subdomain_count', 'is_ip', 'has_port', 'suspicious_word_count', 'has_shortening_service',
                'has_legitimate_domain', 'is_https', 'has_www', 'path_depth', 'query_params_count',
                'has_fragment', 'domain_entropy', 'path_entropy', 'url_entropy', 'has_hex_chars',
                'contains_ip', 'abnormal_subdomain', 'long_domain', 'redirect_patterns',
                'digit_ratio', 'special_char_ratio'
            ]
            features = {name: 0 for name in feature_names}

        return features

def create_comprehensive_dataset():
    """Create a comprehensive dataset with real-world URLs"""

    # Real phishing URLs (these are known phishing patterns, safe to use for training)
    phishing_urls = [
        # Banking/Financial phishing
        "http://secure-paypal-verification.fake-security.com/login",
        "https://chase-bank-security-alert.suspicious.net/verify-account",
        "http://wells-fargo-account-locked.phishing.org/unlock",
        "https://amazon-security-team.fake-amazon.com/suspended-account",
        "http://apple-id-verification-required.scammer.net/verify",
        "https://microsoft-account-unusual-activity.phisher.com/secure",
        "http://ebay-account-limitation-notice.fake-ebay.org/restore",
        "https://192.168.1.100/online-banking/login-verification",
        "http://bit.ly/fake-bank-login-urgent-action-required",
        "https://google-security-alert-suspicious-activity.malicious.com/verify",

        # E-commerce phishing
        "http://amazon-prize-winner.fake-contest.org/claim-now",
        "https://ebay-second-chance-offer.scammer.net/pay-now",
        "http://walmart-gift-card-winner.phishing.com/redeem",
        "https://target-customer-survey.fake-target.org/reward",
        "http://costco-membership-renewal.suspicious.net/renew",

        # Tech company phishing
        "https://facebook-account-will-be-deleted.fake-fb.com/save-account",
        "http://twitter-account-suspended.phisher.org/appeal",
        "https://instagram-copyright-violation.fake-insta.net/dispute",
        "http://youtube-channel-strike.scammer.com/appeal-now",
        "https://linkedin-premium-expired.fake-linkedin.org/renew",

        # Government/Official phishing
        "http://irs-tax-refund-pending.fake-gov.com/claim",
        "https://usps-package-delivery-failed.scammer.net/reschedule",
        "http://dmv-license-renewal-required.phishing.gov.fake/renew",
        "https://social-security-benefits.fake-ssa.org/verify",

        # Random suspicious patterns
        "http://urgent-action-required-immediately.suspicious.com/click-here",
        "https://limited-time-offer-expires-soon.scammer.net/act-now",
        "http://congratulations-you-have-won.fake-lottery.org/claim",
        "https://security-breach-detected.phisher.com/secure-account",
        "http://account-will-be-closed-today.urgent.net/prevent",
        "https://suspicious-login-detected.fake-security.com/verify"
    ]

    # Legitimate URLs
    legitimate_urls = [
        # Major websites
        "https://www.google.com",
        "https://www.facebook.com",
        "https://www.youtube.com",
        "https://www.amazon.com",
        "https://www.wikipedia.org",
        "https://www.twitter.com",
        "https://www.instagram.com",
        "https://www.linkedin.com",
        "https://www.reddit.com",
        "https://www.github.com",

        # E-commerce
        "https://www.ebay.com/itm/example-item",
        "https://www.etsy.com/listing/12345/handmade-item",
        "https://www.walmart.com/ip/product-12345",
        "https://www.target.com/p/product-name/-/A-12345",
        "https://www.bestbuy.com/site/product/12345.p",

        # Tech/Development
        "https://stackoverflow.com/questions/12345/python-question",
        "https://docs.python.org/3/library/urllib.html",
        "https://developer.mozilla.org/en-US/docs/Web/JavaScript",
        "https://www.w3schools.com/html/html_intro.asp",
        "https://github.com/microsoft/vscode",

        # News/Information
        "https://www.bbc.com/news/world-12345",
        "https://www.cnn.com/2024/01/15/politics/news-article",
        "https://www.nytimes.com/2024/01/15/technology/article.html",
        "https://techcrunch.com/2024/01/15/startup-funding-news",

        # Educational
        "https://www.coursera.org/learn/machine-learning",
        "https://www.edx.org/course/introduction-to-computer-science",
        "https://www.khanacademy.org/computing/computer-programming",
        "https://www.udemy.com/course/python-bootcamp",

        # Entertainment
        "https://www.netflix.com/title/12345",
        "https://www.spotify.com/us/premium/",
        "https://www.twitch.tv/username",
        "https://www.hulu.com/series/show-name"
    ]

    # Create more samples by variations
    extended_phishing = []
    extended_legitimate = []

    # Add variations of phishing URLs
    for url in phishing_urls:
        extended_phishing.append(url)
        # Add HTTP variant
        if url.startswith('https://'):
            extended_phishing.append(url.replace('https://', 'http://'))
        # Add www variant
        if 'www.' not in url:
            extended_phishing.append(url.replace('://', '://www.'))

    # Add variations of legitimate URLs
    for url in legitimate_urls:
        extended_legitimate.append(url)
        # Add paths
        extended_legitimate.append(f"{url}/about")
        extended_legitimate.append(f"{url}/contact")
        extended_legitimate.append(f"{url}/help")

    # Combine and create DataFrame
    all_urls = extended_legitimate + extended_phishing
    labels = [0] * len(extended_legitimate) + [1] * len(extended_phishing)

    df = pd.DataFrame({
        'url': all_urls,
        'class': labels
    })

    return df

print("üìä Creating Comprehensive Phishing Dataset...")
df = create_comprehensive_dataset()
print(f"‚úÖ Dataset created successfully! Shape: {df.shape}")
print(f"üìä Class distribution: {df['class'].value_counts().to_dict()}")

# Initialize advanced feature extractor
feature_extractor = AdvancedURLFeatureExtractor()

print("üîß Extracting advanced features from URLs...")
feature_list = []
for url in df['url']:
    features = feature_extractor.extract_features(url)
    feature_list.append(features)

# Create feature DataFrame
X_features = pd.DataFrame(feature_list)
y = df['class']

print(f"‚úÖ Feature extraction completed! Features shape: {X_features.shape}")
print(f"üî¢ Number of features: {len(X_features.columns)}")

# Handle any missing values
X_features = X_features.fillna(0)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_features, y, test_size=0.25, random_state=42, stratify=y
)

# Scale features for algorithms that need it
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nü§ñ Training Multiple Advanced Models...")

# Train multiple models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42)
}

results = {}
trained_models = {}

for name, model in models.items():
    print(f"Training {name}...")

    if name in ['Logistic Regression', 'SVM']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    results[name] = {
        'accuracy': accuracy,
        'predictions': y_pred,
        'probabilities': y_pred_proba,
        'classification_report': classification_report(y_test, y_pred, output_dict=True)
    }
    trained_models[name] = model

    print(f"‚úÖ {name} Accuracy: {accuracy:.4f}")

# Choose best model
best_model_name = max(results.keys(), key=lambda k: results[k]['accuracy'])
best_model = trained_models[best_model_name]
print(f"\nüèÜ Best Model: {best_model_name} (Accuracy: {results[best_model_name]['accuracy']:.4f})")

# Create comprehensive visualizations
print("\nüìä Creating Advanced Visualizations...")

fig = plt.figure(figsize=(24, 18))

# 1. Model Comparison
plt.subplot(4, 4, 1)
model_names = list(results.keys())
accuracies = [results[name]['accuracy'] for name in model_names]
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
bars = plt.bar(model_names, accuracies, color=colors)
plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.xticks(rotation=45)
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')

# 2. Confusion Matrix Heatmap
plt.subplot(4, 4, 2)
cm = confusion_matrix(y_test, results[best_model_name]['predictions'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Legitimate', 'Phishing'],
            yticklabels=['Legitimate', 'Phishing'])
plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')

# 3. ROC Curves
plt.subplot(4, 4, 3)
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
for i, name in enumerate(results.keys()):
    fpr, tpr, _ = roc_curve(y_test, results[name]['probabilities'])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color=colors[i], label=f'{name} (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')
plt.legend()

# 4. Feature Importance
plt.subplot(4, 4, 4)
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    feature_importance = pd.DataFrame({
        'feature': X_features.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False).head(12)

    plt.barh(range(len(feature_importance)), feature_importance['importance'], color='lightcoral')
    plt.yticks(range(len(feature_importance)), feature_importance['feature'])
    plt.xlabel('Importance')
    plt.title('Top 12 Feature Importances', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()

# 5. URL Length Analysis
plt.subplot(4, 4, 5)
legitimate_lengths = X_features[y == 0]['url_length']
phishing_lengths = X_features[y == 1]['url_length']
plt.hist(legitimate_lengths, alpha=0.7, label='Legitimate', bins=30, color='lightgreen', density=True)
plt.hist(phishing_lengths, alpha=0.7, label='Phishing', bins=30, color='lightcoral', density=True)
plt.xlabel('URL Length')
plt.ylabel('Density')
plt.title('URL Length Distribution', fontsize=14, fontweight='bold')
plt.legend()

# 6. Domain Entropy vs Suspicious Words
plt.subplot(4, 4, 6)
colors = ['lightgreen' if label == 0 else 'lightcoral' for label in y]
plt.scatter(X_features['domain_entropy'], X_features['suspicious_word_count'],
           c=colors, alpha=0.7, s=30)
plt.xlabel('Domain Entropy')
plt.ylabel('Suspicious Word Count')
plt.title('Domain Entropy vs Suspicious Words', fontsize=14, fontweight='bold')

# 7. Security Features Analysis
plt.subplot(4, 4, 7)
security_features = ['is_https', 'has_www', 'has_legitimate_domain']
legitimate_security = X_features[y == 0][security_features].mean()
phishing_security = X_features[y == 1][security_features].mean()

x = np.arange(len(security_features))
width = 0.35

plt.bar(x - width/2, legitimate_security, width, label='Legitimate', color='lightgreen')
plt.bar(x + width/2, phishing_security, width, label='Phishing', color='lightcoral')
plt.xlabel('Security Features')
plt.ylabel('Average Presence')
plt.title('Security Features Comparison', fontsize=14, fontweight='bold')
plt.xticks(x, security_features, rotation=45)
plt.legend()

# 8. Path Depth Distribution
plt.subplot(4, 4, 8)
legitimate_depth = X_features[y == 0]['path_depth']
phishing_depth = X_features[y == 1]['path_depth']
plt.hist(legitimate_depth, alpha=0.7, label='Legitimate', bins=15, color='lightgreen', density=True)
plt.hist(phishing_depth, alpha=0.7, label='Phishing', bins=15, color='lightcoral', density=True)
plt.xlabel('Path Depth')
plt.ylabel('Density')
plt.title('Path Depth Distribution', fontsize=14, fontweight='bold')
plt.legend()

# 9. Subdomain Analysis
plt.subplot(4, 4, 9)
subdomain_data = pd.crosstab(y, X_features['subdomain_count'])
subdomain_data.plot(kind='bar', ax=plt.gca(), color=['lightgreen', 'lightcoral'])
plt.title('Subdomain Count Distribution', fontsize=14, fontweight='bold')
plt.xlabel('Class (0=Legitimate, 1=Phishing)')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(['0 Subdomains', '1 Subdomain', '2+ Subdomains'])

# 10. Special Character Analysis
plt.subplot(4, 4, 10)
plt.scatter(X_features['num_hyphens'], X_features['num_dots'],
           c=['lightgreen' if label == 0 else 'lightcoral' for label in y], alpha=0.7)
plt.xlabel('Number of Hyphens')
plt.ylabel('Number of Dots')
plt.title('Special Characters Pattern', fontsize=14, fontweight='bold')

# 11. Model Performance Metrics
plt.subplot(4, 4, 11)
metrics = ['precision', 'recall', 'f1-score']
model_metrics = {}
for model_name in results.keys():
    model_metrics[model_name] = [
        results[model_name]['classification_report']['weighted avg']['precision'],
        results[model_name]['classification_report']['weighted avg']['recall'],
        results[model_name]['classification_report']['weighted avg']['f1-score']
    ]

x = np.arange(len(metrics))
width = 0.2
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

for i, (model_name, values) in enumerate(model_metrics.items()):
    plt.bar(x + i*width, values, width, label=model_name, color=colors[i])

plt.xlabel('Metrics')
plt.ylabel('Score')
plt.title('Model Performance Metrics', fontsize=14, fontweight='bold')
plt.xticks(x + width*1.5, metrics)
plt.legend()

# 12. Prediction Confidence Distribution
plt.subplot(4, 4, 12)
probabilities = results[best_model_name]['probabilities']
plt.hist(probabilities, bins=20, alpha=0.7, color='mediumpurple', density=True)
plt.xlabel('Prediction Confidence (Phishing Probability)')
plt.ylabel('Density')
plt.title('Model Confidence Distribution', fontsize=14, fontweight='bold')
plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Decision Threshold')
plt.legend()

# 13. Feature Correlation Heatmap
plt.subplot(4, 4, 13)
# Select top features for correlation
top_features = X_features.columns[:10]  # First 10 features
corr_matrix = X_features[top_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')

# 14. Class Distribution
plt.subplot(4, 4, 14)
class_counts = y.value_counts()
colors = ['lightgreen', 'lightcoral']
plt.pie(class_counts.values, labels=['Legitimate', 'Phishing'],
        colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Dataset Class Distribution', fontsize=14, fontweight='bold')

# 15. Domain Length vs URL Length
plt.subplot(4, 4, 15)
plt.scatter(X_features['domain_length'], X_features['url_length'],
           c=['lightgreen' if label == 0 else 'lightcoral' for label in y], alpha=0.7)
plt.xlabel('Domain Length')
plt.ylabel('URL Length')
plt.title('Domain vs URL Length', fontsize=14, fontweight='bold')

# 16. Entropy Analysis
plt.subplot(4, 4, 16)
entropy_features = ['domain_entropy', 'path_entropy', 'url_entropy']
legitimate_entropy = X_features[y == 0][entropy_features].mean()
phishing_entropy = X_features[y == 1][entropy_features].mean()

x = np.arange(len(entropy_features))
width = 0.35

plt.bar(x - width/2, legitimate_entropy, width, label='Legitimate', color='lightgreen')
plt.bar(x + width/2, phishing_entropy, width, label='Phishing', color='lightcoral')
plt.xlabel('Entropy Types')
plt.ylabel('Average Entropy')
plt.title('Entropy Analysis', fontsize=14, fontweight='bold')
plt.xticks(x, ['Domain', 'Path', 'URL'])
plt.legend()

plt.tight_layout()
plt.show()

# Print comprehensive results
print("\n" + "="*80)
print("üéØ COMPREHENSIVE RESULTS ANALYSIS")
print("="*80)

for name, result in results.items():
    print(f"\nüìä {name} Results:")
    print(f"Accuracy: {result['accuracy']:.4f}")
    print(f"Precision (Weighted): {result['classification_report']['weighted avg']['precision']:.4f}")
    print(f"Recall (Weighted): {result['classification_report']['weighted avg']['recall']:.4f}")
    print(f"F1-Score (Weighted): {result['classification_report']['weighted avg']['f1-score']:.4f}")

# Feature importance analysis
print(f"\nüìà FEATURE ANALYSIS ({best_model_name})")
print("="*50)
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    feature_importance = pd.DataFrame({
        'feature': X_features.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("Top 15 most important features:")
    print(feature_importance.head(15).to_string(index=False, float_format='%.4f'))

    print(f"\nTotal features: {len(X_features.columns)}")
    print(f"Features with importance > 0.01: {len(feature_importance[feature_importance['importance'] > 0.01])}")

# Save models and components
print("\nüíæ Saving Advanced Models...")
with open('best_phishing_model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

with open('all_models.pkl', 'wb') as f:
    pickle.dump(trained_models, f)

with open('advanced_feature_extractor.pkl', 'wb') as f:
    pickle.dump(feature_extractor, f)

with open('feature_scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print("‚úÖ All models and components saved successfully!")

# Enhanced URL Prediction Function
def predict_url_advanced(url, model=None, extractor=None, scaler=None, model_name=None):
    """
    Advanced URL safety prediction with detailed analysis
    """
    if model is None:
        model = best_model
    if extractor is None:
        extractor = feature_extractor
    if model_name is None:
        model_name = best_model_name

    try:
        # Extract features
        features = extractor.extract_features(url)
        feature_vector = pd.DataFrame([features])

        # Scale if needed
        if model_name in ['Logistic Regression', 'SVM']:
            feature_vector_scaled = scaler.transform(feature_vector)
            prediction = model.predict(feature_vector_scaled)[0]
            probabilities = model.predict_proba(feature_vector_scaled)[0]
        else:
            prediction = model.predict(feature_vector)[0]
            probabilities = model.predict_proba(feature_vector)[0]

        # Analyze key features
        key_features = {
            'URL Length': features['url_length'],
            'Domain Length': features['domain_length'],
            'Suspicious Words': features['suspicious_word_count'],
            'HTTPS': 'Yes' if features['is_https'] else 'No',
            'Subdomain Count': features['subdomain_count'],
            'Special Characters': features['num_special_chars'],
            'Domain Entropy': round(features['domain_entropy'], 3),
            'Has Legitimate Domain': 'Yes' if features['has_legitimate_domain'] else 'No'
        }

        # Risk factors
        risk_factors = []
        if features['url_length'] > 75:
            risk_factors.append("Very long URL")
        if features['suspicious_word_count'] > 2:
            risk_factors.append("Multiple suspicious words")
        if features['is_https'] == 0:
            risk_factors.append("No HTTPS encryption")
        if features['subdomain_count'] > 2:
            risk_factors.append("Too many subdomains")
        if features['has_shortening_service']:
            risk_factors.append("URL shortening service")
        if features['is_ip']:
            risk_factors.append("IP address instead of domain")
        if features['domain_entropy'] > 4:
            risk_factors.append("High domain randomness")

        # Safety indicators
        safety_indicators = []
        if features['has_legitimate_domain']:
            safety_indicators.append("Recognized legitimate domain")
        if features['is_https']:
            safety_indicators.append("HTTPS encryption")
        if features['suspicious_word_count'] == 0:
            safety_indicators.append("No suspicious keywords")
        if features['url_length'] < 50:
            safety_indicators.append("Reasonable URL length")

        result = {
            'url': url,
            'prediction': 'Phishing ‚ö†Ô∏è' if prediction == 1 else 'Legitimate ‚úÖ',
            'confidence': max(probabilities),
            'phishing_probability': probabilities[1],
            'legitimate_probability': probabilities[0],
            'risk_level': 'HIGH' if probabilities[1] > 0.8 else 'MEDIUM' if probabilities[1] > 0.5 else 'LOW',
            'key_features': key_features,
            'risk_factors': risk_factors,
            'safety_indicators': safety_indicators
        }

        return result

    except Exception as e:
        return {
            'url': url,
            'prediction': 'Error in prediction',
            'error': str(e)
        }

# Test with comprehensive examples
print("\nüß™ COMPREHENSIVE URL TESTING")
print("="*80)

test_urls = [
    # Legitimate URLs
    "https://www.google.com/search?q=python+programming",
    "https://github.com/microsoft/vscode/releases",
    "https://stackoverflow.com/questions/tagged/python",
    "https://docs.python.org/3/library/urllib.html",
    "https://www.amazon.com/dp/B08N5WRWNW",

    # Suspicious/Phishing URLs
    "http://secure-paypal-verification.suspicious-domain.com/login/verify",
    "https://amazon-security-alert.fake-amazon.org/suspended-account",
    "http://192.168.1.100/online-banking/secure-login",
    "https://bit.ly/urgent-account-verification-required",
    "http://apple-id-locked-suspicious-activity.phisher.net/unlock-now",

    # Edge cases
    "https://subdomain.another.deep.nested.domain.com/very/long/path/here",
    "http://legitimate-looking-bank.com/login",
    "https://www.paypal.com",  # Real PayPal for comparison
]

for i, url in enumerate(test_urls, 1):
    print(f"\nüîç Test {i}: {url}")
    print("-" * 80)

    result = predict_url_advanced(url)

    if 'error' not in result:
        print(f"üéØ Prediction: {result['prediction']}")
        print(f"üìä Confidence: {result['confidence']:.3f}")
        print(f"‚ö†Ô∏è  Risk Level: {result['risk_level']}")
        print(f"üî¢ Phishing Probability: {result['phishing_probability']:.3f}")

        print("\nüìã Key Features:")
        for feature, value in result['key_features'].items():
            print(f"   ‚Ä¢ {feature}: {value}")

        if result['risk_factors']:
            print(f"\n‚ö†Ô∏è  Risk Factors ({len(result['risk_factors'])}):")
            for factor in result['risk_factors']:
                print(f"   ‚Ä¢ {factor}")

        if result['safety_indicators']:
            print(f"\n‚úÖ Safety Indicators ({len(result['safety_indicators'])}):")
            for indicator in result['safety_indicators']:
                print(f"   ‚Ä¢ {indicator}")
    else:
        print(f"‚ùå Error: {result['error']}")

# Model comparison summary
print(f"\nüìä MODEL PERFORMANCE SUMMARY")
print("="*60)
print(f"Best Model: {best_model_name}")
print(f"Best Accuracy: {results[best_model_name]['accuracy']:.4f}")
print(f"Total Features: {len(X_features.columns)}")
print(f"Training Samples: {len(X_train)}")
print(f"Testing Samples: {len(X_test)}")

print("\nüèÜ All Model Accuracies:")
for name, result in sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True):
    print(f"   {name}: {result['accuracy']:.4f}")

# Feature engineering summary
print(f"\nüîß FEATURE ENGINEERING SUMMARY")
print("="*50)
print("Advanced features extracted:")
print("‚Ä¢ Length-based features (5)")
print("‚Ä¢ Character count features (10)")
print("‚Ä¢ Domain analysis features (3)")
print("‚Ä¢ Suspicious pattern features (3)")
print("‚Ä¢ Security indicator features (2)")
print("‚Ä¢ URL structure features (3)")
print("‚Ä¢ Entropy and randomness features (3)")
print("‚Ä¢ Advanced pattern features (5)")
print("‚Ä¢ Ratio features (2)")
print(f"Total: {len(X_features.columns)} features")

print("\nüéâ ADVANCED PHISHING DETECTION SYSTEM COMPLETE!")
print("="*60)
print("‚úÖ Multiple models trained and compared")
print("‚úÖ Advanced feature extraction implemented")
print("‚úÖ Comprehensive visualizations generated")
print("‚úÖ Detailed prediction analysis available")
print("‚úÖ All models and components saved")
print("\nüîç Use predict_url_advanced(url) for detailed URL analysis!")
print("üìä Check the generated plots for comprehensive insights!")

# Save sample predictions for documentation
sample_results = []
for url in test_urls[:5]:  # First 5 URLs
    result = predict_url_advanced(url)
    if 'error' not in result:
        sample_results.append({
            'url': result['url'],
            'prediction': result['prediction'],
            'confidence': result['confidence'],
            'risk_level': result['risk_level']
        })

sample_df = pd.DataFrame(sample_results)
sample_df.to_csv('sample_predictions.csv', index=False)
print("\nüíæ Sample predictions saved to 'sample_predictions.csv'")

# Create a summary report
summary_report = f"""
# Phishing Detection System - Performance Report

## Dataset Information
- Total URLs: {len(df)}
- Legitimate URLs: {len(df[df['class'] == 0])}
- Phishing URLs: {len(df[df['class'] == 1])}
- Features Extracted: {len(X_features.columns)}

## Model Performance
"""

for name, result in sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True):
    summary_report += f"- {name}: {result['accuracy']:.4f} accuracy\n"

summary_report += f"""
## Best Model: {best_model_name}
- Accuracy: {results[best_model_name]['accuracy']:.4f}
- Precision: {results[best_model_name]['classification_report']['weighted avg']['precision']:.4f}
- Recall: {results[best_model_name]['classification_report']['weighted avg']['recall']:.4f}
- F1-Score: {results[best_model_name]['classification_report']['weighted avg']['f1-score']:.4f}

## Key Features
The system analyzes {len(X_features.columns)} different URL characteristics including:
- URL structure and length analysis
- Domain characteristics and entropy
- Security indicators (HTTPS, suspicious words)
- Pattern recognition for phishing attempts
"""

with open('performance_report.md', 'w') as f:
    f.write(summary_report)

print("üìÑ Performance report saved to 'performance_report.md'")